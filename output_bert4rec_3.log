Using device: cuda
Data directory: /home/ubuntu/Projects/CSE258_assignment2/data
Loaded 7,793,069 raw interactions
Prepared 334725 user histories for training
Validation targets: 334725, Test targets: 334725
Users available for evaluation: 334725
Using 20000 users for consistent evaluation subset
MostPopular -> Hit@10: 0.0586, NDCG@10: 0.0290
Markov Chain -> Hit@10: 0.0654, NDCG@10: 0.0345
Item-KNN -> Hit@10: 0.0717, NDCG@10: 0.0364
Encoded histories prepared for 334704 users
Note: All deep learning methods will use this same user set for fair comparison
Filtered train_histories: 334704 users (same as encoded_histories)
Original train_histories: 334725 users
Users removed: 21
Filtered eval_users: 20000 users (matching filtered train_histories)
Recreated eval sets: 20000 users
Building BERT4Rec dataset with data augmentation...
Created 1,673,520 training samples
Data augmentation factor: 5x
Items masked per sequence: 20
Number of batches: 6,538
/home/ubuntu/miniconda3/envs/CSE258/lib/python3.10/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training BERT4Rec (this will take 1-2 hours)...
======================================================================
⚠️  WARNING: This model needs 200+ epochs to work properly!
======================================================================
Epoch 1/200 - Loss: 1.9495 - Best: 1.9495 - Time: 0.6min
Epoch 10/200 - Loss: 0.8927 - Best: 0.8927 - Time: 6.3min
Epoch 20/200 - Loss: 0.8749 - Best: 0.8749 - Time: 12.9min
Epoch 30/200 - Loss: 0.8628 - Best: 0.8628 - Time: 19.4min
Epoch 40/200 - Loss: 0.8534 - Best: 0.8534 - Time: 25.8min
Epoch 50/200 - Loss: 0.8445 - Best: 0.8445 - Time: 32.2min
Epoch 60/200 - Loss: 0.8388 - Best: 0.8388 - Time: 38.5min
Epoch 70/200 - Loss: 0.8353 - Best: 0.8353 - Time: 44.8min
Epoch 80/200 - Loss: 0.8337 - Best: 0.8332 - Time: 51.1min
Epoch 90/200 - Loss: 0.8350 - Best: 0.8332 - Time: 57.5min
Epoch 100/200 - Loss: 0.8410 - Best: 0.8332 - Time: 64.2min
Epoch 110/200 - Loss: 0.8495 - Best: 0.8332 - Time: 70.6min
Epoch 120/200 - Loss: 0.8624 - Best: 0.8332 - Time: 77.4min
Epoch 130/200 - Loss: 0.8793 - Best: 0.8332 - Time: 84.5min
Epoch 140/200 - Loss: 0.8981 - Best: 0.8332 - Time: 91.4min
Epoch 150/200 - Loss: 0.9222 - Best: 0.8332 - Time: 98.0min
Epoch 160/200 - Loss: 0.9513 - Best: 0.8332 - Time: 104.4min
Epoch 170/200 - Loss: 0.9877 - Best: 0.8332 - Time: 110.8min
Epoch 180/200 - Loss: 1.0330 - Best: 0.8332 - Time: 117.4min
Epoch 190/200 - Loss: 1.0901 - Best: 0.8332 - Time: 124.1min
Epoch 200/200 - Loss: 1.1553 - Best: 0.8332 - Time: 130.6min
/home/ubuntu/Projects/CSE258_assignment2/bert4rec_run.py:936: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load("pretrained_models/bert4rec_best.pth")

✅ Training complete! Best model from epoch 78 with loss 0.8332
Total training time: 130.6 minutes
Figure saved to bert4rec_training_loss.png
Final loss: 1.1553
Best loss: 0.8332
Total epochs: 200
Evaluation function ready
Evaluating BERT4Rec model...

BERT4Rec (20000 users) -> Hit@10: 0.0088, NDCG@10: 0.0046
