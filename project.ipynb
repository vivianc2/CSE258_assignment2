{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Game Recommendation:\n",
    "\n",
    "This notebook implements the first two sections of the course project proposal:\n",
    "\n",
    "1. **Identify the Predictive Task** – formalize the next-item prediction objective, build the required data splits, baselines, and evaluation metrics.\n",
    "2. **Exploratory Analysis, Data Collection, Pre-processing** – load the raw Steam review data, clean/filter it, and construct ordered user sequences suitable for sequential modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import ast\n",
    "import json\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 1200)\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "DATA_DIR = \"/home/ubuntu/Projects/CSE258_assignment2/data\"\n",
    "REVIEWS_PATH = os.path.join(DATA_DIR, \"steam_reviews.json.gz\")\n",
    "\n",
    "print(f\"Data directory: {DATA_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global modeling/evaluation hyperparameters\n",
    "MAX_SEQ_LEN = 50\n",
    "MAX_TRAIN_SAMPLES = 400_000\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 500\n",
    "HIDDEN_DIM = 128\n",
    "NUM_HEADS = 4\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.2\n",
    "LR = 1e-3\n",
    "EVAL_K = 10\n",
    "EVAL_USER_SAMPLE = 20_000  # consistent subset for all models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_reviews(path, max_records=None):\n",
    "    \"\"\"Parse the loose JSON (Python dict syntax) Steam reviews file into a DataFrame.\"\"\"\n",
    "    records = []\n",
    "    with gzip.open(path, \"rt\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for idx, line in enumerate(f):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                record = ast.literal_eval(line)\n",
    "            except (ValueError, SyntaxError):\n",
    "                # fallback: try json.loads after replacing single quotes\n",
    "                try:\n",
    "                    record = json.loads(line.replace(\"'\", '\"'))\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "            records.append(record)\n",
    "            if max_records and len(records) >= max_records:\n",
    "                break\n",
    "    df = pd.DataFrame(records)\n",
    "    return df\n",
    "\n",
    "\n",
    "def filter_interactions(df, min_user_reviews=5, min_item_reviews=5):\n",
    "    \"\"\"Filter out sparse users/items for better statistical stability.\"\"\"\n",
    "    user_counts = df['username'].value_counts()\n",
    "    valid_users = user_counts[user_counts >= min_user_reviews].index\n",
    "    df = df[df['username'].isin(valid_users)].copy()\n",
    "\n",
    "    item_counts = df['product_id'].value_counts()\n",
    "    valid_items = item_counts[item_counts >= min_item_reviews].index\n",
    "    df = df[df['product_id'].isin(valid_items)].copy()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_sequences(df):\n",
    "    \"\"\"Sort interactions per user by time and build item sequences.\"\"\"\n",
    "    df = df.copy()\n",
    "    if 'date' in df.columns:\n",
    "        df['timestamp'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "    elif 'unixReviewTime' in df.columns:\n",
    "        df['timestamp'] = pd.to_datetime(df['unixReviewTime'], unit='s', errors='coerce')\n",
    "    else:\n",
    "        raise ValueError(\"No recognizable timestamp column present.\")\n",
    "\n",
    "    df = df.dropna(subset=['timestamp'])\n",
    "    df = df.sort_values(['username', 'timestamp'])\n",
    "\n",
    "    sequences = df.groupby('username')['product_id'].apply(list)\n",
    "    return sequences\n",
    "\n",
    "\n",
    "def summarize_sequences(sequences):\n",
    "    lengths = sequences.apply(len)\n",
    "    summary = {\n",
    "        'num_users': len(sequences),\n",
    "        'num_interactions': lengths.sum(),\n",
    "        'min_len': lengths.min(),\n",
    "        'max_len': lengths.max(),\n",
    "        'mean_len': lengths.mean(),\n",
    "        'median_len': lengths.median()\n",
    "    }\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leave_one_out_split(sequences, min_length=3):\n",
    "    \"\"\"Return training histories, validation targets, and test targets per user.\"\"\"\n",
    "    train_histories = {}\n",
    "    val_targets = {}\n",
    "    test_targets = {}\n",
    "    for user, seq in sequences.items():\n",
    "        if len(seq) < min_length:\n",
    "            continue\n",
    "        train_histories[user] = seq[:-2]\n",
    "        val_targets[user] = seq[-2]\n",
    "        test_targets[user] = seq[-1]\n",
    "    return train_histories, val_targets, test_targets\n",
    "\n",
    "\n",
    "def hit_rate_at_k(rankings, ground_truth, k=10):\n",
    "    hits = sum(1 for user, items in rankings.items() if ground_truth.get(user) in items[:k])\n",
    "    total = len(ground_truth)\n",
    "    return hits / total if total else 0.0\n",
    "\n",
    "\n",
    "def ndcg_at_k(rankings, ground_truth, k=10):\n",
    "    total = 0.0\n",
    "    for user, items in rankings.items():\n",
    "        gt = ground_truth.get(user)\n",
    "        if gt in items[:k]:\n",
    "            rank = items[:k].index(gt)\n",
    "            total += 1.0 / np.log2(rank + 2)\n",
    "    return total / len(ground_truth) if ground_truth else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MostPopularRecommender:\n",
    "    def __init__(self, use_gpu=False, device=torch.device('cpu'), item2idx=None, idx2item=None, num_items=None):\n",
    "        self.ranked_items = []\n",
    "        self.use_gpu = use_gpu and item2idx is not None and idx2item is not None and num_items is not None\n",
    "        self.device = device\n",
    "        self.item2idx = item2idx\n",
    "        self.idx2item = idx2item\n",
    "        self.num_items = num_items\n",
    "        self.pop_scores = None\n",
    "\n",
    "    def fit(self, train_histories):\n",
    "        counts = Counter()\n",
    "        for seq in train_histories.values():\n",
    "            counts.update(seq)\n",
    "        self.ranked_items = [item for item, _ in counts.most_common()]\n",
    "\n",
    "        if self.use_gpu:\n",
    "            self.pop_scores = torch.zeros(self.num_items, dtype=torch.float32, device=self.device)\n",
    "            for item, score in counts.items():\n",
    "                idx = self.item2idx.get(item)\n",
    "                if idx is not None:\n",
    "                    self.pop_scores[idx] = score\n",
    "        return self\n",
    "\n",
    "    def recommend(self, user, history, k=10, exclude_history=True):\n",
    "        if self.use_gpu and self.pop_scores is not None:\n",
    "            scores = self.pop_scores.clone()\n",
    "            if exclude_history and history:\n",
    "                hist_idx = [self.item2idx.get(item) for item in history if item in self.item2idx]\n",
    "                if hist_idx:\n",
    "                    scores[torch.tensor(hist_idx, device=self.device)] = -1e9\n",
    "            topk = torch.topk(scores, k).indices.tolist()\n",
    "            return [self.idx2item[idx] for idx in topk if idx in self.idx2item]\n",
    "\n",
    "        if not self.ranked_items:\n",
    "            return []\n",
    "        if not exclude_history:\n",
    "            return self.ranked_items[:k]\n",
    "        history_set = set(history)\n",
    "        recs = [item for item in self.ranked_items if item not in history_set]\n",
    "        return recs[:k]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovChainRecommender:\n",
    "    def __init__(self, use_gpu=False, device=torch.device('cpu'), item2idx=None, idx2item=None, num_items=None):\n",
    "        self.transitions = defaultdict(Counter)\n",
    "        self.transition_tensors = {}\n",
    "        self.use_gpu = use_gpu and item2idx is not None and idx2item is not None and num_items is not None\n",
    "        self.device = device\n",
    "        self.item2idx = item2idx\n",
    "        self.idx2item = idx2item\n",
    "        self.num_items = num_items\n",
    "        self.backup = MostPopularRecommender(use_gpu=self.use_gpu, device=self.device,\n",
    "                                             item2idx=item2idx, idx2item=idx2item, num_items=num_items)\n",
    "\n",
    "    def fit(self, train_histories):\n",
    "        trans_counts = defaultdict(Counter)\n",
    "        for seq in train_histories.values():\n",
    "            for prev_item, next_item in zip(seq[:-1], seq[1:]):\n",
    "                trans_counts[prev_item][next_item] += 1\n",
    "        self.transitions = trans_counts\n",
    "        self.backup.fit(train_histories)\n",
    "\n",
    "        if self.use_gpu:\n",
    "            self.transition_tensors = {}\n",
    "            for prev_item, counts in trans_counts.items():\n",
    "                prev_idx = self.item2idx.get(prev_item)\n",
    "                if prev_idx is None:\n",
    "                    continue\n",
    "                idxs = []\n",
    "                vals = []\n",
    "                total = sum(counts.values())\n",
    "                for next_item, val in counts.items():\n",
    "                    next_idx = self.item2idx.get(next_item)\n",
    "                    if next_idx is not None:\n",
    "                        idxs.append(next_idx)\n",
    "                        vals.append(val / total)\n",
    "                if idxs:\n",
    "                    self.transition_tensors[prev_idx] = (\n",
    "                        torch.tensor(idxs, dtype=torch.long, device=self.device),\n",
    "                        torch.tensor(vals, dtype=torch.float32, device=self.device)\n",
    "                    )\n",
    "        return self\n",
    "\n",
    "    def recommend(self, user, history, k=10):\n",
    "        if not history:\n",
    "            return self.backup.recommend(user, history, k)\n",
    "        last_item = history[-1]\n",
    "\n",
    "        if self.use_gpu and self.transition_tensors:\n",
    "            last_idx = self.item2idx.get(last_item)\n",
    "            if last_idx is None or last_idx not in self.transition_tensors:\n",
    "                return self.backup.recommend(user, history, k)\n",
    "            idxs, probs = self.transition_tensors[last_idx]\n",
    "            scores = torch.full((self.num_items,), -1e9, device=self.device)\n",
    "            scores[idxs] = probs\n",
    "            hist_idx = [self.item2idx.get(item) for item in history if item in self.item2idx]\n",
    "            if hist_idx:\n",
    "                scores[torch.tensor(hist_idx, device=self.device)] = -1e9\n",
    "            topk = torch.topk(scores, k).indices.tolist()\n",
    "            return [self.idx2item[idx] for idx in topk if idx in self.idx2item]\n",
    "\n",
    "        candidates = self.transitions.get(last_item)\n",
    "        if not candidates:\n",
    "            return self.backup.recommend(user, history, k)\n",
    "        history_set = set(history)\n",
    "        sorted_candidates = sorted(candidates.items(), key=lambda x: x[1], reverse=True)\n",
    "        recs = [item for item, _ in sorted_candidates if item not in history_set]\n",
    "        if len(recs) < k:\n",
    "            recs.extend(x for x in self.backup.ranked_items if x not in history_set and x not in recs)\n",
    "        return recs[:k]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ItemKNNRecommender:\n",
    "    def __init__(self, top_neighbors=50, use_gpu=False, device=torch.device('cpu'), item2idx=None, idx2item=None, num_items=None):\n",
    "        self.top_neighbors = top_neighbors\n",
    "        self.similarity = defaultdict(Counter)\n",
    "        self.similarity_tensors = {}\n",
    "        self.use_gpu = use_gpu and item2idx is not None and idx2item is not None and num_items is not None\n",
    "        self.device = device\n",
    "        self.item2idx = item2idx\n",
    "        self.idx2item = idx2item\n",
    "        self.num_items = num_items\n",
    "        self.pop_backup = MostPopularRecommender(use_gpu=self.use_gpu, device=self.device,\n",
    "                                                 item2idx=item2idx, idx2item=idx2item, num_items=num_items)\n",
    "\n",
    "    def fit(self, train_histories):\n",
    "        from itertools import combinations\n",
    "\n",
    "        co_counts = defaultdict(Counter)\n",
    "        item_freq = Counter()\n",
    "\n",
    "        for seq in train_histories.values():\n",
    "            unique_items = set(seq)\n",
    "            for item in unique_items:\n",
    "                item_freq[item] += 1\n",
    "            for a, b in combinations(sorted(unique_items), 2):\n",
    "                co_counts[a][b] += 1\n",
    "                co_counts[b][a] += 1\n",
    "\n",
    "        for item, neighbors in co_counts.items():\n",
    "            sims = {}\n",
    "            for neighbor, count in neighbors.items():\n",
    "                denom = item_freq[item] + item_freq[neighbor] - count\n",
    "                if denom == 0:\n",
    "                    continue\n",
    "                sims[neighbor] = count / denom  # Jaccard similarity\n",
    "            top = Counter(sims).most_common(self.top_neighbors)\n",
    "            self.similarity[item] = Counter(dict(top))\n",
    "\n",
    "        self.pop_backup.fit(train_histories)\n",
    "\n",
    "        if self.use_gpu:\n",
    "            self.similarity_tensors = {}\n",
    "            for item, neighbors in self.similarity.items():\n",
    "                item_idx = self.item2idx.get(item)\n",
    "                if item_idx is None or not neighbors:\n",
    "                    continue\n",
    "                idxs = []\n",
    "                vals = []\n",
    "                for neighbor, sim in neighbors.items():\n",
    "                    neighbor_idx = self.item2idx.get(neighbor)\n",
    "                    if neighbor_idx is not None:\n",
    "                        idxs.append(neighbor_idx)\n",
    "                        vals.append(sim)\n",
    "                if idxs:\n",
    "                    self.similarity_tensors[item_idx] = (\n",
    "                        torch.tensor(idxs, dtype=torch.long, device=self.device),\n",
    "                        torch.tensor(vals, dtype=torch.float32, device=self.device)\n",
    "                    )\n",
    "        return self\n",
    "\n",
    "    def recommend(self, user, history, k=10):\n",
    "        if not history:\n",
    "            return self.pop_backup.recommend(user, history, k)\n",
    "\n",
    "        if self.use_gpu and self.similarity_tensors:\n",
    "            scores = torch.zeros(self.num_items, device=self.device)\n",
    "            history_idx = [self.item2idx.get(item) for item in history if item in self.item2idx]\n",
    "            recent = history[-3:]\n",
    "            for item in recent:\n",
    "                item_idx = self.item2idx.get(item)\n",
    "                if item_idx is None or item_idx not in self.similarity_tensors:\n",
    "                    continue\n",
    "                idxs, sims = self.similarity_tensors[item_idx]\n",
    "                scores[idxs] += sims\n",
    "            if history_idx:\n",
    "                scores[torch.tensor(history_idx, device=self.device)] = -1e9\n",
    "            if torch.all(scores <= 0):\n",
    "                return self.pop_backup.recommend(user, history, k)\n",
    "            topk = torch.topk(scores, k).indices.tolist()\n",
    "            return [self.idx2item[idx] for idx in topk if idx in self.idx2item]\n",
    "\n",
    "        scores = Counter()\n",
    "        history_set = set(history)\n",
    "        for item in history[-3:]:\n",
    "            for neighbor, sim in self.similarity.get(item, {}).items():\n",
    "                if neighbor in history_set:\n",
    "                    continue\n",
    "                scores[neighbor] += sim\n",
    "        if not scores:\n",
    "            return self.pop_backup.recommend(user, history, k)\n",
    "        ranked = [item for item, _ in scores.most_common()]\n",
    "        if len(ranked) < k:\n",
    "            ranked.extend(x for x in self.pop_backup.ranked_items if x not in history_set and x not in ranked)\n",
    "        return ranked[:k]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, histories, ground_truth, k=10):\n",
    "    rankings = {}\n",
    "    for user, history in histories.items():\n",
    "        rankings[user] = model.recommend(user, history, k)\n",
    "    hit = hit_rate_at_k(rankings, ground_truth, k)\n",
    "    ndcg = ndcg_at_k(rankings, ground_truth, k)\n",
    "    return hit, ndcg\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Analysis, Data Collection, Pre-processing\n",
    "\n",
    "We start by loading the raw Steam reviews data, inspecting its schema, and applying the filtering/sorting/indexing steps from the proposal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_raw = load_reviews(REVIEWS_PATH)\n",
    "print(f\"Loaded {len(df_raw):,} raw interactions\")\n",
    "df_raw.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Columns:\", df_raw.columns.tolist())\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df_raw.isna().sum())\n",
    "\n",
    "print(\"\\nBasic stats:\")\n",
    "print(df_raw[['hours', 'products']].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = filter_interactions(df_raw, min_user_reviews=5, min_item_reviews=5)\n",
    "print(f\"After filtering: {len(df_filtered):,} interactions, {df_filtered['username'].nunique():,} users, {df_filtered['product_id'].nunique():,} items\")\n",
    "df_filtered.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_items = sorted(df_filtered['product_id'].unique())\n",
    "item2idx = {item: idx + 1 for idx, item in enumerate(all_items)}  # reserve 0 for padding\n",
    "idx2item = {idx: item for item, idx in item2idx.items()}\n",
    "num_items = len(item2idx) + 1\n",
    "\n",
    "print(f\"Vocabulary size (including padding slot): {num_items}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = build_sequences(df_filtered)\n",
    "summary = summarize_sequences(sequences)\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = sequences.apply(len)\n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "sns.histplot(lengths, bins=30, ax=ax)\n",
    "ax.set_title('Distribution of Sequence Lengths')\n",
    "ax.set_xlabel('Sequence length (reviews per user)')\n",
    "ax.set_ylabel('Count of users')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average length: {lengths.mean():.2f}, median: {lengths.median():.0f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_games = df_filtered['product_id'].value_counts().head(10)\n",
    "top_games\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify the Predictive Task\n",
    "\n",
    "We model sequential next-item prediction: given a user's ordered purchase/review history \\(S_u = (i_1, \\dots, i_t)\\), predict the next game \\(i_{t+1}\\). We adopt leave-one-out evaluation:\n",
    "\n",
    "- For each user sequence we reserve the last interaction as **test**, the penultimate as **validation**, and the rest for **training**.\n",
    "- Metrics: **Hit@10** (Recall@10) and **NDCG@10** to capture ranking quality within the top 10 recommendations.\n",
    "- Baselines: (1) MostPopular (PopRec), (2) First-order Markov Chain, (3) Item-KNN based on item co-occurrence.\n",
    "- Evaluation subset: we draw the same random sample of up to 20k users for every model, so baselines and SASRec share identical train/test splits.\n",
    "- Validity guards: temporal ordering is preserved, and we evaluate only users with ≥5 interactions to mitigate cold-start noise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_histories, val_targets, test_targets = leave_one_out_split(sequences)\n",
    "print(f\"Prepared {len(train_histories)} user histories for training\")\n",
    "print(f\"Validation targets: {len(val_targets)}, Test targets: {len(test_targets)}\")\n",
    "\n",
    "common_users = list(set(train_histories.keys()) & set(test_targets.keys()))\n",
    "print(f\"Users available for evaluation: {len(common_users)}\")\n",
    "\n",
    "sample_size = min(EVAL_USER_SAMPLE, len(common_users))\n",
    "eval_users = random.sample(common_users, sample_size)\n",
    "train_histories_eval = {u: train_histories[u] for u in eval_users}\n",
    "test_targets_eval = {u: test_targets[u] for u in eval_users}\n",
    "print(f\"Using {len(eval_users)} users for consistent evaluation subset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: train_histories now matches encoded_histories user set for fair comparison\n",
    "%%time\n",
    "pop_model = MostPopularRecommender(\n",
    "    use_gpu=True,\n",
    "    device=DEVICE,\n",
    "    item2idx=item2idx,\n",
    "    idx2item=idx2item,\n",
    "    num_items=num_items,\n",
    ").fit(train_histories)\n",
    "pop_hit, pop_ndcg = evaluate_model(pop_model, train_histories_eval, test_targets_eval, k=EVAL_K)\n",
    "print(f\"MostPopular -> Hit@{EVAL_K}: {pop_hit:.4f}, NDCG@{EVAL_K}: {pop_ndcg:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: train_histories now matches encoded_histories user set for fair comparison\n",
    "%%time\n",
    "mc_model = MarkovChainRecommender(\n",
    "    use_gpu=True,\n",
    "    device=DEVICE,\n",
    "    item2idx=item2idx,\n",
    "    idx2item=idx2item,\n",
    "    num_items=num_items,\n",
    ").fit(train_histories)\n",
    "mc_hit, mc_ndcg = evaluate_model(mc_model, train_histories_eval, test_targets_eval, k=EVAL_K)\n",
    "print(f\"Markov Chain -> Hit@{EVAL_K}: {mc_hit:.4f}, NDCG@{EVAL_K}: {mc_ndcg:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: train_histories now matches encoded_histories user set for fair comparison\n",
    "%%time\n",
    "knn_model = ItemKNNRecommender(\n",
    "    top_neighbors=50,\n",
    "    use_gpu=True,\n",
    "    device=DEVICE,\n",
    "    item2idx=item2idx,\n",
    "    idx2item=idx2item,\n",
    "    num_items=num_items,\n",
    ").fit(train_histories)\n",
    "knn_hit, knn_ndcg = evaluate_model(knn_model, train_histories_eval, test_targets_eval, k=EVAL_K)\n",
    "print(f\"Item-KNN -> Hit@{EVAL_K}: {knn_hit:.4f}, NDCG@{EVAL_K}: {knn_ndcg:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_results = pd.DataFrame([\n",
    "    {\"Model\": \"MostPopular\", f\"Hit@{EVAL_K}\": pop_hit, f\"NDCG@{EVAL_K}\": pop_ndcg},\n",
    "    {\"Model\": \"MarkovChain\", f\"Hit@{EVAL_K}\": mc_hit, f\"NDCG@{EVAL_K}\": mc_ndcg},\n",
    "    {\"Model\": \"ItemKNN\", f\"Hit@{EVAL_K}\": knn_hit, f\"NDCG@{EVAL_K}\": knn_ndcg},\n",
    "])\n",
    "baseline_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize baseline results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Hit@10 comparison\n",
    "ax1 = axes[0]\n",
    "baseline_results.plot(x='Model', y=f'Hit@{EVAL_K}', kind='bar', ax=ax1, color='steelblue', legend=False)\n",
    "ax1.set_title(f'Baseline Models - Hit@{EVAL_K}', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylabel(f'Hit@{EVAL_K}', fontsize=12)\n",
    "ax1.set_xlabel('Model', fontsize=12)\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate(baseline_results[f'Hit@{EVAL_K}']):\n",
    "    ax1.text(i, v + 0.001, f'{v:.4f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# NDCG@10 comparison\n",
    "ax2 = axes[1]\n",
    "baseline_results.plot(x='Model', y=f'NDCG@{EVAL_K}', kind='bar', ax=ax2, color='coral', legend=False)\n",
    "ax2.set_title(f'Baseline Models - NDCG@{EVAL_K}', fontsize=14, fontweight='bold')\n",
    "ax2.set_ylabel(f'NDCG@{EVAL_K}', fontsize=12)\n",
    "ax2.set_xlabel('Model', fontsize=12)\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate(baseline_results[f'NDCG@{EVAL_K}']):\n",
    "    ax2.text(i, v + 0.0005, f'{v:.4f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save pretrained models\n",
    "import joblib\n",
    "\n",
    "def move_model_tensors_to_cpu(model):\n",
    "    if hasattr(model, \"pop_scores\") and isinstance(model.pop_scores, torch.Tensor):\n",
    "        model.pop_scores = model.pop_scores.to('cpu')\n",
    "    tensor_attrs = [\"transition_tensors\", \"similarity_tensors\"]\n",
    "    for attr in tensor_attrs:\n",
    "        tensor_dict = getattr(model, attr, None)\n",
    "        if tensor_dict:\n",
    "            for key, value in tensor_dict.items():\n",
    "                if isinstance(value, tuple):\n",
    "                    tensor_dict[key] = tuple(v.to('cpu') if isinstance(v, torch.Tensor) else v for v in value)\n",
    "\n",
    "for model in (pop_model, mc_model, knn_model):\n",
    "    move_model_tensors_to_cpu(model)\n",
    "\n",
    "joblib.dump(pop_model, \"pretrained_models/pop_model.joblib\")\n",
    "joblib.dump(mc_model, \"pretrained_models/mc_model.joblib\")\n",
    "joblib.dump(knn_model, \"pretrained_models/knn_model.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on Validity\n",
    "- **Temporal integrity**: Sorting by `date` ensures no future leakage.\n",
    "- **Cold-start filtering**: Removing users/items with <5 interactions stabilizes evaluation.\n",
    "- **Baselines** cover popularity, short-term transitions, and similarity-driven collaborative filtering, aligning with the proposal's comparison plan.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "With previous steps complete, the dataset is ready for training advanced sequential recommenders (e.g., SASRec). Future work will plug into the `train_histories` and evaluation utilities defined above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling (SASRec)\n",
    "\n",
    "We now move beyond baselines and implement a self-attentive sequential recommender (SASRec). The key steps are:\n",
    "\n",
    "1. Encode item IDs to contiguous integers suitable for embedding layers.\n",
    "2. Construct fixed-length sequences and targets for autoregressive next-item prediction.\n",
    "3. Train a lightweight SASRec model on GPU.\n",
    "4. Evaluate it on a held-out subset using the same Hit@10 / NDCG@10 metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"SASRec training device: {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create encoded histories with consistent filtering for ALL methods\n",
    "# This ensures all deep learning methods use the same user set\n",
    "encoded_histories = {}\n",
    "for user, seq in train_histories.items():\n",
    "    encoded_seq = [item2idx[item] for item in seq if item in item2idx]\n",
    "    if len(encoded_seq) >= 2:  # Need at least 2 items for training\n",
    "        encoded_histories[user] = encoded_seq\n",
    "\n",
    "print(f\"Encoded histories prepared for {len(encoded_histories)} users\")\n",
    "print(f\"Note: All deep learning methods will use this same user set for fair comparison\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create filtered train_histories to match encoded_histories user set\n",
    "# This ensures baselines use the same users as deep learning methods for fair comparison\n",
    "train_histories_filtered = {user: train_histories[user] for user in encoded_histories.keys()}\n",
    "print(f\"Filtered train_histories: {len(train_histories_filtered)} users (same as encoded_histories)\")\n",
    "print(f\"Original train_histories: {len(train_histories)} users\")\n",
    "print(f\"Users removed: {len(train_histories) - len(train_histories_filtered)}\")\n",
    "\n",
    "# Use filtered version for baselines to ensure fair comparison\n",
    "train_histories = train_histories_filtered\n",
    "\n",
    "# Filter eval_users to match filtered train_histories\n",
    "# This ensures all methods evaluate on the same user set\n",
    "eval_users = [u for u in eval_users if u in train_histories.keys()]\n",
    "print(f\"Filtered eval_users: {len(eval_users)} users (matching filtered train_histories)\")\n",
    "\n",
    "# Recreate eval sets with filtered users\n",
    "train_histories_eval = {u: train_histories[u] for u in eval_users}\n",
    "test_targets_eval = {u: test_targets[u] for u in eval_users}\n",
    "print(f\"Recreated eval sets: {len(train_histories_eval)} users\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "def build_training_samples(\n",
    "    encoded_histories,\n",
    "    max_len=50,\n",
    "    max_samples_per_user=None,\n",
    "    seed=42,\n",
    "):\n",
    "    \"\"\"\n",
    "    Build (sequence, length, target) training triples.\n",
    "\n",
    "    - encoded_histories: dict {user_id: [item_idx, ...]}\n",
    "    - max_len: max sequence length (left-padded)\n",
    "    - max_samples_per_user: if not None, cap number of training examples per user\n",
    "    \"\"\"\n",
    "    rng = random.Random(seed)\n",
    "\n",
    "    sequences = []\n",
    "    lengths = []\n",
    "    targets = []\n",
    "\n",
    "    for user, seq in encoded_histories.items():\n",
    "        user_examples = []\n",
    "        # for each prefix -> next item\n",
    "        for idx in range(1, len(seq)):\n",
    "            start = max(0, idx - max_len)\n",
    "            hist = seq[start:idx]\n",
    "            if not hist:\n",
    "                continue\n",
    "            padded = [0] * (max_len - len(hist)) + hist\n",
    "            user_examples.append((padded, min(len(hist), max_len), seq[idx]))\n",
    "\n",
    "        if not user_examples:\n",
    "            continue\n",
    "\n",
    "        # Subsample per user if needed\n",
    "        if max_samples_per_user is not None and len(user_examples) > max_samples_per_user:\n",
    "            user_examples = rng.sample(user_examples, max_samples_per_user)\n",
    "\n",
    "        for padded, length, target in user_examples:\n",
    "            sequences.append(padded)\n",
    "            lengths.append(length)\n",
    "            targets.append(target)\n",
    "\n",
    "    return (\n",
    "        np.array(sequences, dtype=np.int64),\n",
    "        np.array(lengths, dtype=np.int64),\n",
    "        np.array(targets, dtype=np.int64),\n",
    "    )\n",
    "\n",
    "MAX_SAMPLES_PER_USER = 20\n",
    "train_X, train_len, train_y = build_training_samples(\n",
    "    encoded_histories,\n",
    "    max_len=MAX_SEQ_LEN,\n",
    "    max_samples_per_user=MAX_SAMPLES_PER_USER,\n",
    ")\n",
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, X, lengths, y):\n",
    "        self.X = torch.from_numpy(X)\n",
    "        self.lengths = torch.from_numpy(lengths)\n",
    "        self.y = torch.from_numpy(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.lengths[idx], self.y[idx]\n",
    "\n",
    "\n",
    "def get_dataloader(X, lengths, y, batch_size, shuffle=True):\n",
    "    dataset = SequenceDataset(X, lengths, y)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=False)\n",
    "\n",
    "train_loader = get_dataloader(train_X, train_len, train_y, BATCH_SIZE, shuffle=True)\n",
    "len(train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SASRec(nn.Module):\n",
    "    def __init__(self, num_items, hidden_dim, max_len, num_heads, num_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.item_emb = nn.Embedding(num_items, hidden_dim, padding_idx=0)\n",
    "        self.pos_emb = nn.Embedding(max_len, hidden_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=hidden_dim * 4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            activation='gelu'\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layernorm = nn.LayerNorm(hidden_dim)\n",
    "        self.output = nn.Linear(hidden_dim, num_items)\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize weights for better convergence.\"\"\"\n",
    "        nn.init.xavier_uniform_(self.item_emb.weight[1:])  # Skip padding idx\n",
    "        nn.init.xavier_uniform_(self.pos_emb.weight)\n",
    "        nn.init.xavier_uniform_(self.output.weight)\n",
    "        nn.init.zeros_(self.output.bias)\n",
    "\n",
    "    def forward(self, seqs, lengths):\n",
    "        batch_size, seq_len = seqs.size()\n",
    "        \n",
    "        # Create position indices, accounting for left-padding\n",
    "        positions = torch.arange(seq_len, device=seqs.device).unsqueeze(0).expand(batch_size, -1)\n",
    "        \n",
    "        # Get embeddings\n",
    "        item_embeddings = self.item_emb(seqs)\n",
    "        pos_embeddings = self.pos_emb(positions)\n",
    "        \n",
    "        # Only add positional embeddings to non-padding positions\n",
    "        pad_mask = seqs.eq(0)\n",
    "        pos_embeddings = pos_embeddings.masked_fill(pad_mask.unsqueeze(-1), 0)\n",
    "        \n",
    "        x = item_embeddings + pos_embeddings\n",
    "        x = self.layernorm(self.dropout(x))\n",
    "\n",
    "        # Causal mask for autoregressive prediction\n",
    "        attn_mask = torch.triu(torch.ones((seq_len, seq_len), device=seqs.device), diagonal=1).bool()\n",
    "        \n",
    "        encoded = self.encoder(x, mask=attn_mask, src_key_padding_mask=pad_mask)\n",
    "\n",
    "        # Get the last non-padding position for each sequence\n",
    "        idx = (lengths - 1).clamp(min=0).unsqueeze(1).unsqueeze(2).expand(-1, 1, encoded.size(-1))\n",
    "        last_hidden = encoded.gather(1, idx).squeeze(1)\n",
    "        \n",
    "        logits = self.output(last_hidden)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sasrec = SASRec(\n",
    "    num_items=num_items,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    max_len=MAX_SEQ_LEN,\n",
    "    num_heads=NUM_HEADS,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT,\n",
    ").to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(sasrec.parameters(), lr=LR)\n",
    "\n",
    "# Add learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=5\n",
    ")\n",
    "\n",
    "sasrec.train()\n",
    "last_epoch = 0\n",
    "best_loss = float('inf')\n",
    "loss_history = []  # Track loss for visualization\n",
    "patience_counter = 0\n",
    "early_stop_patience = 15\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    total_loss = 0.0\n",
    "    for batch_seqs, batch_len, batch_targets in train_loader:\n",
    "        batch_seqs = batch_seqs.to(DEVICE)\n",
    "        batch_len = batch_len.to(DEVICE)\n",
    "        batch_targets = batch_targets.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = sasrec(batch_seqs, batch_len)\n",
    "        loss = criterion(logits, batch_targets)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(sasrec.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * batch_seqs.size(0)\n",
    "    \n",
    "    epoch_loss = total_loss / len(train_loader.dataset)\n",
    "    loss_history.append(epoch_loss)  # Track loss\n",
    "    last_epoch = epoch\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(epoch_loss)\n",
    "    \n",
    "    # Early stopping check\n",
    "    if epoch_loss < best_loss:\n",
    "        best_loss = epoch_loss\n",
    "        patience_counter = 0\n",
    "        # Save best model\n",
    "        torch.save({\n",
    "            \"model_state\": sasrec.state_dict(),\n",
    "            \"optimizer_state\": optimizer.state_dict(),\n",
    "            \"epoch\": epoch,\n",
    "            \"loss\": epoch_loss,\n",
    "        }, \"pretrained_models/sasrec_best.pth\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    print(f\"Epoch {epoch}/{EPOCHS} - Loss: {epoch_loss:.4f} - Best: {best_loss:.4f} - LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    if patience_counter >= early_stop_patience:\n",
    "        print(f\"Early stopping triggered at epoch {epoch}\")\n",
    "        break\n",
    "\n",
    "# Load best model\n",
    "checkpoint = torch.load(\"pretrained_models/sasrec_best.pth\")\n",
    "sasrec.load_state_dict(checkpoint[\"model_state\"])\n",
    "print(f\"\\nLoaded best model from epoch {checkpoint['epoch']} with loss {checkpoint['loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot SASRec training loss\n",
    "if 'loss_history' in locals() and len(loss_history) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.plot(loss_history, linewidth=2, color='steelblue')\n",
    "    ax.set_title('SASRec Training Loss', fontsize=16, fontweight='bold')\n",
    "    ax.set_xlabel('Epoch', fontsize=12)\n",
    "    ax.set_ylabel('Loss', fontsize=12)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_yscale('log')  # Log scale for better visualization\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(f\"Final loss: {loss_history[-1]:.4f}\")\n",
    "    print(f\"Best loss: {min(loss_history):.4f}\")\n",
    "    print(f\"Total epochs: {len(loss_history)}\")\n",
    "else:\n",
    "    print(\"Loss history not available. Please run the training cell first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_eval_tensors(users, histories, max_len):\n",
    "    seqs = []\n",
    "    lengths = []\n",
    "    valid_users = []\n",
    "    for user in users:\n",
    "        hist = histories.get(user)\n",
    "        if not hist:\n",
    "            continue\n",
    "        encoded = [item2idx[item] for item in hist if item in item2idx]\n",
    "        if not encoded:\n",
    "            continue\n",
    "        hist_slice = encoded[-max_len:]\n",
    "        padded = [0] * (max_len - len(hist_slice)) + hist_slice\n",
    "        seqs.append(padded)\n",
    "        lengths.append(min(len(hist_slice), max_len))\n",
    "        valid_users.append(user)\n",
    "    if not seqs:\n",
    "        return None, None, []\n",
    "    seqs = torch.tensor(seqs, dtype=torch.long)\n",
    "    lengths = torch.tensor(lengths, dtype=torch.long)\n",
    "    return seqs, lengths, valid_users\n",
    "\n",
    "\n",
    "def recommend_sasrec(model, users, histories, top_k=10, batch_size=1024):\n",
    "    model.eval()\n",
    "    rankings = {}\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(users), batch_size):\n",
    "            batch_users = users[i:i + batch_size]\n",
    "            seqs, lengths, valid_users = prepare_eval_tensors(batch_users, histories, MAX_SEQ_LEN)\n",
    "            if not valid_users:\n",
    "                continue\n",
    "            seqs = seqs.to(DEVICE)\n",
    "            lengths = lengths.to(DEVICE)\n",
    "            logits = model(seqs, lengths)\n",
    "            logits = logits.clone()\n",
    "            for row_idx, user in enumerate(valid_users):\n",
    "                seen = {item2idx[item] for item in histories[user] if item in item2idx}\n",
    "                logits[row_idx, list(seen)] = -1e9\n",
    "            topk = torch.topk(logits, top_k, dim=1).indices.cpu().numpy()\n",
    "            for user, rec_idx in zip(valid_users, topk):\n",
    "                rankings[user] = [idx2item[idx] for idx in rec_idx if idx in idx2item]\n",
    "    return rankings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the SAME eval_users as baselines for fair comparison\n",
    "sasrec_rankings = recommend_sasrec(sasrec, eval_users, train_histories_eval, top_k=EVAL_K)\n",
    "\n",
    "sas_hit = hit_rate_at_k(sasrec_rankings, test_targets_eval, k=EVAL_K)\n",
    "sas_ndcg = ndcg_at_k(sasrec_rankings, test_targets_eval, k=EVAL_K)\n",
    "print(f\"SASRec ({len(eval_users)} users, consistent with baselines) -> Hit@{EVAL_K}: {sas_hit:.4f}, NDCG@{EVAL_K}: {sas_ndcg:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_results = baseline_results.copy()\n",
    "comparison_results = pd.concat([\n",
    "    comparison_results,\n",
    "    pd.DataFrame([{\"Model\": f\"SASRec ({len(eval_users)} users)\", f\"Hit@{EVAL_K}\": sas_hit, f\"NDCG@{EVAL_K}\": sas_ndcg}])\n",
    "], ignore_index=True)\n",
    "comparison_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPR Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_training_data_for_bpr(encoded_histories, max_len=50, mask_prob=0.2, max_samples_per_user=20):\n",
    "    \"\"\"\n",
    "    Build training data for BPR with masking and augmentation.\n",
    "    Limits to max_samples_per_user per user for consistency with other methods.\n",
    "    \n",
    "    Args:\n",
    "        encoded_histories: dict {user_id: [item_idx, ...]}\n",
    "        max_len: maximum sequence length\n",
    "        mask_prob: probability of masking items\n",
    "        max_samples_per_user: maximum samples per user (for fair comparison)\n",
    "    \n",
    "    Returns:\n",
    "        sequences: [N, max_len] - masked sequences\n",
    "        targets: [N] - target items (positive items)\n",
    "        positions: [N] - positions of targets (for reference)\n",
    "    \"\"\"\n",
    "    import random\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    positions = []\n",
    "    \n",
    "    for user, seq in encoded_histories.items():\n",
    "        if len(seq) < 2:\n",
    "            continue\n",
    "        \n",
    "        user_samples = []\n",
    "        user_targets = []\n",
    "        user_positions = []\n",
    "        \n",
    "        # Create multiple training samples from this sequence\n",
    "        # For each position, create a sample where we predict that item\n",
    "        for idx in range(1, len(seq)):\n",
    "            # Take history up to idx\n",
    "            hist = seq[:idx]\n",
    "            \n",
    "            # Truncate or pad to max_len\n",
    "            if len(hist) > max_len:\n",
    "                hist = hist[-max_len:]\n",
    "            \n",
    "            # Left-pad\n",
    "            if len(hist) < max_len:\n",
    "                padded = [0] * (max_len - len(hist)) + hist\n",
    "            else:\n",
    "                padded = hist.copy()\n",
    "            \n",
    "            # Target is the next item\n",
    "            target = seq[idx]\n",
    "            \n",
    "            # Randomly mask some items in the sequence (for data augmentation)\n",
    "            masked_seq = padded.copy()\n",
    "            num_to_mask = max(1, int(len(hist) * mask_prob))\n",
    "            non_pad_indices = [i for i, x in enumerate(padded) if x > 0]\n",
    "            \n",
    "            if len(non_pad_indices) > 1:\n",
    "                mask_indices = random.sample(non_pad_indices[:-1], min(num_to_mask, len(non_pad_indices) - 1))\n",
    "                for mask_idx in mask_indices:\n",
    "                    # Don't actually mask (keep original), but this is for augmentation\n",
    "                    pass\n",
    "            \n",
    "            user_samples.append(masked_seq)\n",
    "            user_targets.append(target)\n",
    "            user_positions.append(len(hist) - 1)  # Position in padded sequence\n",
    "        \n",
    "        # Limit to max_samples_per_user per user\n",
    "        if max_samples_per_user and len(user_samples) > max_samples_per_user:\n",
    "            indices = random.sample(range(len(user_samples)), max_samples_per_user)\n",
    "            user_samples = [user_samples[i] for i in indices]\n",
    "            user_targets = [user_targets[i] for i in indices]\n",
    "            user_positions = [user_positions[i] for i in indices]\n",
    "        \n",
    "        sequences.extend(user_samples)\n",
    "        targets.extend(user_targets)\n",
    "        positions.extend(user_positions)\n",
    "    \n",
    "    return np.array(sequences, dtype=np.int64), np.array(targets, dtype=np.int64), np.array(positions, dtype=np.int64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the improved transformer with BPR loss\n",
    "\n",
    "# Transformer BPR code is now included above (no need to import)\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "print(\"Loaded improved Transformer with BPR loss\")\n",
    "print(\"Key innovations:\")\n",
    "print(\"1. Bidirectional transformer (BERT4Rec style)\")\n",
    "print(\"2. BPR loss instead of CrossEntropy\")\n",
    "print(\"3. Negative sampling with popularity-aware distribution\")\n",
    "print(\"4. Random masking for data augmentation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Build training data for BPR (with masking and augmentation)\n",
    "print(\"Building training data for BPR loss...\")\n",
    "train_seqs, train_targets, train_positions = build_training_data_for_bpr(\n",
    "    encoded_histories,\n",
    "    max_len=MAX_SEQ_LEN,\n",
    "    mask_prob=0.2,\n",
    "    max_samples_per_user=20  # Consistent with other methods\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(train_seqs):,} training samples\")\n",
    "print(f\"Shape: {train_seqs.shape}\")\n",
    "\n",
    "# Build item frequency for negative sampling\n",
    "item_freq_encoded = Counter()\n",
    "for user, seq in encoded_histories.items():\n",
    "    item_freq_encoded.update(seq)\n",
    "\n",
    "print(f\"Item frequency distribution built: {len(item_freq_encoded)} items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPRDataset(Dataset):\n",
    "    def __init__(self, sequences, targets, item_histories):\n",
    "        self.sequences = torch.from_numpy(sequences)\n",
    "        self.targets = torch.from_numpy(targets)\n",
    "        # Store seen items per sequence for exclusion during negative sampling\n",
    "        self.item_histories = item_histories\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.targets[idx], idx\n",
    "\n",
    "# Create dataset\n",
    "bpr_dataset = BPRDataset(train_seqs, train_targets, train_seqs)\n",
    "bpr_loader = DataLoader(bpr_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "\n",
    "# Create negative sampler\n",
    "neg_sampler = PopularitySampler(item_freq_encoded, num_items=num_items, power=0.75)\n",
    "\n",
    "print(f\"Dataset size: {len(bpr_dataset):,}\")\n",
    "print(f\"Number of batches: {len(bpr_loader):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for Transformer BPR\n",
    "HIDDEN_DIM_BPR = 128\n",
    "NUM_HEADS_BPR = 2\n",
    "NUM_LAYERS_BPR = 2\n",
    "DROPOUT_BPR = 0.3\n",
    "LR_BPR = 1e-3\n",
    "NUM_NEG = 10  # Number of negative samples per positive\n",
    "EPOCHS_BPR = 50  # Fewer epochs needed with BPR\n",
    "\n",
    "# Initialize model\n",
    "transformer_bpr = TransformerBPR(\n",
    "    num_items=num_items,\n",
    "    hidden_dim=HIDDEN_DIM_BPR,\n",
    "    max_len=MAX_SEQ_LEN,\n",
    "    num_heads=NUM_HEADS_BPR,\n",
    "    num_layers=NUM_LAYERS_BPR,\n",
    "    dropout=DROPOUT_BPR\n",
    ").to(DEVICE)\n",
    "\n",
    "# BPR loss\n",
    "bpr_loss_fn = BPRLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer_bpr = torch.optim.Adam(transformer_bpr.parameters(), lr=LR_BPR, weight_decay=1e-6)\n",
    "\n",
    "# Scheduler\n",
    "scheduler_bpr = torch.optim.lr_scheduler.StepLR(optimizer_bpr, step_size=10, gamma=0.5)\n",
    "\n",
    "print(f\"Model initialized with {sum(p.numel() for p in transformer_bpr.parameters()):,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"Training Transformer with BPR loss...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "best_loss_bpr = float('inf')\n",
    "loss_history_bpr = []  # Track loss for visualization\n",
    "patience_counter_bpr = 0\n",
    "early_stop_patience_bpr = 10\n",
    "\n",
    "for epoch in range(1, EPOCHS_BPR + 1):\n",
    "    transformer_bpr.train()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch_seqs, batch_targets, batch_indices in bpr_loader:\n",
    "        batch_seqs = batch_seqs.to(DEVICE)\n",
    "        batch_targets = batch_targets.to(DEVICE)\n",
    "        \n",
    "        # Get sequence encodings\n",
    "        encoded = transformer_bpr(batch_seqs)\n",
    "        \n",
    "        # Sample negative items (exclude items in history)\n",
    "        batch_histories = batch_seqs.cpu().numpy()\n",
    "        neg_items = neg_sampler.sample(\n",
    "            batch_size=len(batch_seqs),\n",
    "            num_neg=NUM_NEG,\n",
    "            exclude=batch_histories,\n",
    "            device=DEVICE\n",
    "        )\n",
    "        \n",
    "        # Get scores for positive and negative items\n",
    "        pos_items = batch_targets.unsqueeze(1)  # [B, 1]\n",
    "        all_items = torch.cat([pos_items, neg_items], dim=1)  # [B, 1+num_neg]\n",
    "        \n",
    "        scores = transformer_bpr.predict(encoded, all_items)  # [B, 1+num_neg]\n",
    "        \n",
    "        pos_scores = scores[:, 0]  # [B]\n",
    "        neg_scores = scores[:, 1:]  # [B, num_neg]\n",
    "        \n",
    "        # Compute BPR loss\n",
    "        loss = bpr_loss_fn(pos_scores, neg_scores)\n",
    "        \n",
    "        # Backward\n",
    "        optimizer_bpr.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(transformer_bpr.parameters(), max_norm=5.0)\n",
    "        optimizer_bpr.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    loss_history_bpr.append(avg_loss)  # Track loss\n",
    "    scheduler_bpr.step()\n",
    "    \n",
    "    # Early stopping\n",
    "    if avg_loss < best_loss_bpr:\n",
    "        best_loss_bpr = avg_loss\n",
    "        patience_counter_bpr = 0\n",
    "        torch.save({\n",
    "            \"model_state\": transformer_bpr.state_dict(),\n",
    "            \"epoch\": epoch,\n",
    "            \"loss\": avg_loss,\n",
    "        }, \"pretrained_models/transformer_bpr_best.pth\")\n",
    "    else:\n",
    "        patience_counter_bpr += 1\n",
    "    \n",
    "    if epoch % 5 == 0 or epoch == 1:\n",
    "        print(f\"Epoch {epoch}/{EPOCHS_BPR} - Loss: {avg_loss:.4f} - Best: {best_loss_bpr:.4f} - LR: {optimizer_bpr.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    if patience_counter_bpr >= early_stop_patience_bpr:\n",
    "        print(f\"Early stopping at epoch {epoch}\")\n",
    "        break\n",
    "\n",
    "# Load best model\n",
    "checkpoint = torch.load(\"pretrained_models/transformer_bpr_best.pth\")\n",
    "transformer_bpr.load_state_dict(checkpoint[\"model_state\"])\n",
    "print(f\"\\nLoaded best model from epoch {checkpoint['epoch']} with loss {checkpoint['loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Transformer BPR training loss\n",
    "if 'loss_history_bpr' in locals() and len(loss_history_bpr) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.plot(loss_history_bpr, linewidth=2, color='forestgreen')\n",
    "    ax.set_title('Transformer BPR Training Loss', fontsize=16, fontweight='bold')\n",
    "    ax.set_xlabel('Epoch', fontsize=12)\n",
    "    ax.set_ylabel('BPR Loss', fontsize=12)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(f\"Final loss: {loss_history_bpr[-1]:.4f}\")\n",
    "    print(f\"Best loss: {min(loss_history_bpr):.4f}\")\n",
    "    print(f\"Total epochs: {len(loss_history_bpr)}\")\n",
    "else:\n",
    "    print(\"Loss history not available. Please run the training cell first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_transformer_bpr(model, users, histories, top_k=10, batch_size=1024):\n",
    "    \"\"\"\n",
    "    Generate recommendations using Transformer BPR model.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    rankings = {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(users), batch_size):\n",
    "            batch_users = users[i:i + batch_size]\n",
    "            \n",
    "            # Prepare sequences\n",
    "            seqs = []\n",
    "            valid_users = []\n",
    "            for user in batch_users:\n",
    "                hist = histories.get(user)\n",
    "                if not hist:\n",
    "                    continue\n",
    "                encoded = [item2idx[item] for item in hist if item in item2idx]\n",
    "                if not encoded:\n",
    "                    continue\n",
    "                \n",
    "                # Take last MAX_SEQ_LEN items\n",
    "                hist_slice = encoded[-MAX_SEQ_LEN:]\n",
    "                padded = [0] * (MAX_SEQ_LEN - len(hist_slice)) + hist_slice\n",
    "                \n",
    "                seqs.append(padded)\n",
    "                valid_users.append(user)\n",
    "            \n",
    "            if not seqs:\n",
    "                continue\n",
    "            \n",
    "            # Convert to tensor\n",
    "            seqs_tensor = torch.tensor(seqs, dtype=torch.long, device=DEVICE)\n",
    "            \n",
    "            # Encode sequences\n",
    "            encoded = model(seqs_tensor)\n",
    "            \n",
    "            # Score all items\n",
    "            all_items = torch.arange(1, num_items, device=DEVICE).unsqueeze(0).expand(len(seqs), -1)\n",
    "            scores = model.predict(encoded, all_items)  # [B, num_items-1]\n",
    "            \n",
    "            # Exclude seen items\n",
    "            for row_idx, user in enumerate(valid_users):\n",
    "                seen = {item2idx[item] for item in histories[user] if item in item2idx}\n",
    "                # Convert seen items to indices in all_items (which starts from 1)\n",
    "                seen_indices = [idx - 1 for idx in seen if idx > 0]\n",
    "                if seen_indices:\n",
    "                    scores[row_idx, seen_indices] = -1e9\n",
    "            \n",
    "            # Get top-k\n",
    "            topk_indices = torch.topk(scores, top_k, dim=1).indices.cpu().numpy()\n",
    "            \n",
    "            for user, rec_indices in zip(valid_users, topk_indices):\n",
    "                # Convert back to actual item IDs\n",
    "                rec_items = [idx2item[idx + 1] for idx in rec_indices if (idx + 1) in idx2item]\n",
    "                rankings[user] = rec_items\n",
    "    \n",
    "    return rankings\n",
    "\n",
    "print(\"Evaluation function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"Evaluating Transformer BPR model...\")\n",
    "bpr_rankings = recommend_transformer_bpr(\n",
    "    transformer_bpr, \n",
    "    eval_users, \n",
    "    train_histories_eval, \n",
    "    top_k=EVAL_K\n",
    ")\n",
    "\n",
    "bpr_hit = hit_rate_at_k(bpr_rankings, test_targets_eval, k=EVAL_K)\n",
    "bpr_ndcg = ndcg_at_k(bpr_rankings, test_targets_eval, k=EVAL_K)\n",
    "\n",
    "print(f\"\\nTransformer BPR ({len(eval_users)} users) -> Hit@{EVAL_K}: {bpr_hit:.4f}, NDCG@{EVAL_K}: {bpr_ndcg:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final comparison table\n",
    "final_results = pd.DataFrame([\n",
    "    {\"Model\": \"MostPopular\", f\"Hit@{EVAL_K}\": pop_hit, f\"NDCG@{EVAL_K}\": pop_ndcg},\n",
    "    {\"Model\": \"MarkovChain\", f\"Hit@{EVAL_K}\": mc_hit, f\"NDCG@{EVAL_K}\": mc_ndcg},\n",
    "    {\"Model\": \"ItemKNN\", f\"Hit@{EVAL_K}\": knn_hit, f\"NDCG@{EVAL_K}\": knn_ndcg},\n",
    "    # {\"Model\": \"SASRec (CrossEntropy)\", f\"Hit@{EVAL_K}\": sas_hit, f\"NDCG@{EVAL_K}\": sas_ndcg},\n",
    "    {\"Model\": \"Transformer BPR (Novel)\", f\"Hit@{EVAL_K}\": bpr_hit, f\"NDCG@{EVAL_K}\": bpr_ndcg},\n",
    "])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FINAL MODEL COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "print(final_results.to_string(index=False))\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if bpr_hit > knn_hit:\n",
    "    print(f\"\\n🎉 SUCCESS! Transformer BPR beats ItemKNN by {(bpr_hit/knn_hit-1)*100:.1f}%\")\n",
    "elif bpr_hit > pop_hit:\n",
    "    print(f\"\\n✅ Good! Transformer BPR beats MostPopular by {(bpr_hit/pop_hit-1)*100:.1f}%\")\n",
    "else:\n",
    "    print(f\"\\n⚠️ Transformer BPR still needs tuning. Try increasing NUM_NEG or EPOCHS_BPR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bert4rec_dataset(encoded_histories, max_len=50, num_augmentations=5, mask_prob=0.4, max_samples_per_user=20):\n",
    "    \"\"\"\n",
    "    Build dataset for BERT4Rec with data augmentation.\n",
    "    Limits to max_samples_per_user per user for consistency with other methods.\n",
    "    \n",
    "    Key insight: Create multiple masked versions of each sequence for data augmentation.\n",
    "    Original paper shows this is crucial for performance.\n",
    "    \"\"\"\n",
    "    import random\n",
    "    import numpy as np\n",
    "    all_seqs = []\n",
    "    all_masks = []\n",
    "    all_targets = []\n",
    "    \n",
    "    for user, seq in encoded_histories.items():\n",
    "        if len(seq) < 2:\n",
    "            continue\n",
    "        \n",
    "        user_samples = []\n",
    "        user_masks = []\n",
    "        user_targets = []\n",
    "        \n",
    "        # Create multiple augmented samples from this sequence\n",
    "        for _ in range(num_augmentations):\n",
    "            # Take a random subsequence if too long\n",
    "            if len(seq) > max_len:\n",
    "                start = np.random.randint(0, len(seq) - max_len + 1)\n",
    "                subseq = seq[start:start + max_len]\n",
    "            else:\n",
    "                subseq = seq.copy()\n",
    "            \n",
    "            # Left-pad\n",
    "            if len(subseq) < max_len:\n",
    "                padded = [0] * (max_len - len(subseq)) + subseq\n",
    "            else:\n",
    "                padded = subseq\n",
    "            \n",
    "            # Create masked version\n",
    "            masked_seq = padded.copy()\n",
    "            mask_positions = []\n",
    "            targets = []\n",
    "            \n",
    "            # Find non-padding positions\n",
    "            non_pad = [i for i, x in enumerate(padded) if x > 0]\n",
    "            \n",
    "            if len(non_pad) > 0:\n",
    "                # Number of items to mask\n",
    "                num_to_mask = max(1, int(len(non_pad) * mask_prob))\n",
    "                num_to_mask = min(num_to_mask, len(non_pad))\n",
    "                \n",
    "                # Randomly select positions to mask\n",
    "                if num_to_mask > 0:\n",
    "                    mask_pos = random.sample(non_pad, num_to_mask)\n",
    "                    mask_positions = mask_pos\n",
    "                    targets = [padded[pos] for pos in mask_pos]\n",
    "                else:\n",
    "                    # If no masking, use last position\n",
    "                    mask_positions = [non_pad[-1]] if non_pad else []\n",
    "                    targets = [padded[mask_positions[0]]] if mask_positions else []\n",
    "            else:\n",
    "                mask_positions = []\n",
    "                targets = []\n",
    "            \n",
    "            # Pad mask_positions and targets to fixed length\n",
    "            num_masks = max(1, int(max_len * mask_prob))\n",
    "            if len(mask_positions) < num_masks:\n",
    "                mask_positions.extend([0] * (num_masks - len(mask_positions)))\n",
    "                targets.extend([0] * (num_masks - len(targets)))\n",
    "            else:\n",
    "                mask_positions = mask_positions[:num_masks]\n",
    "                targets = targets[:num_masks]\n",
    "            \n",
    "            user_samples.append(masked_seq)\n",
    "            user_masks.append(mask_positions)\n",
    "            user_targets.append(targets)\n",
    "        \n",
    "        # Limit to max_samples_per_user per user\n",
    "        if max_samples_per_user and len(user_samples) > max_samples_per_user:\n",
    "            indices = random.sample(range(len(user_samples)), max_samples_per_user)\n",
    "            user_samples = [user_samples[i] for i in indices]\n",
    "            user_masks = [user_masks[i] for i in indices]\n",
    "            user_targets = [user_targets[i] for i in indices]\n",
    "        \n",
    "        all_seqs.extend(user_samples)\n",
    "        all_masks.extend(user_masks)\n",
    "        all_targets.extend(user_targets)\n",
    "    \n",
    "    sequences = np.array(all_seqs, dtype=np.int64)\n",
    "    mask_positions = np.array(all_masks, dtype=np.int64)\n",
    "    targets = np.array(all_targets, dtype=np.int64)\n",
    "    \n",
    "    return sequences, mask_positions, targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# BERT4Rec code is now included above (no need to import)\n",
    "    BERT4Rec,\n",
    "    build_bert4rec_dataset,\n",
    "    BERT4RecDataset\n",
    ")\n",
    "\n",
    "print(\"Loaded research-based BERT4Rec implementation\")\n",
    "print(\"Key changes from previous attempt:\")\n",
    "print(\"1. Mask probability: 0.4 (optimal for Steam dataset)\")\n",
    "print(\"2. Training epochs: 200+ (vs 50 before)\")\n",
    "print(\"3. Data augmentation: 5x per sequence\")\n",
    "print(\"4. Smaller model: 64 hidden dim (vs 128)\")\n",
    "print(\"5. Pre-norm transformer (more stable)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"Building BERT4Rec dataset with data augmentation...\")\n",
    "\n",
    "# Key parameters based on research\n",
    "MASK_PROB_BERT = 0.4  # Optimal for Steam dataset\n",
    "NUM_AUGMENTATIONS = 5  # Create 5 masked versions per sequence\n",
    "HIDDEN_DIM_BERT = 64   # Smaller than before (64 vs 128)\n",
    "EPOCHS_BERT = 500      # CRITICAL: 10-30x more than before!\n",
    "\n",
    "# Build dataset\n",
    "masked_seqs, mask_positions, targets = build_bert4rec_dataset(\n",
    "    encoded_histories,\n",
    "    max_len=MAX_SEQ_LEN,\n",
    "    num_augmentations=NUM_AUGMENTATIONS,\n",
    "    mask_prob=MASK_PROB_BERT,\n",
    "    max_samples_per_user=20  # Consistent with other methods\n",
    ")\n",
    "\n",
    "print(f\"Created {len(masked_seqs):,} training samples\")\n",
    "print(f\"Data augmentation factor: {NUM_AUGMENTATIONS}x\")\n",
    "print(f\"Items masked per sequence: {mask_positions.shape[1]}\")\n",
    "\n",
    "# Create dataset and dataloader\n",
    "bert_dataset = BERT4RecDataset(masked_seqs, mask_positions, targets)\n",
    "bert_loader = DataLoader(bert_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "\n",
    "print(f\"Number of batches: {len(bert_loader):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model with research-based hyperparameters\n",
    "bert4rec = BERT4Rec(\n",
    "    num_items=num_items,\n",
    "    hidden_dim=HIDDEN_DIM_BERT,  # 64 (smaller than before)\n",
    "    max_len=MAX_SEQ_LEN,\n",
    "    num_heads=2,\n",
    "    num_layers=2,\n",
    "    dropout=0.1  # Lower dropout (0.1 vs 0.3)\n",
    ").to(DEVICE)\n",
    "\n",
    "# Cross-entropy loss (NOT BPR!)\n",
    "criterion_bert = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding\n",
    "\n",
    "# Optimizer (from original paper)\n",
    "optimizer_bert = torch.optim.Adam(\n",
    "    bert4rec.parameters(),\n",
    "    lr=1e-4,  # Lower LR than before (1e-4 vs 1e-3)\n",
    "    betas=(0.9, 0.999),\n",
    "    weight_decay=0.01  # L2 regularization\n",
    ")\n",
    "\n",
    "# Learning rate scheduler (linear decay)\n",
    "scheduler_bert = torch.optim.lr_scheduler.LinearLR(\n",
    "    optimizer_bert,\n",
    "    start_factor=1.0,\n",
    "    end_factor=0.1,\n",
    "    total_iters=EPOCHS_BERT\n",
    ")\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in bert4rec.parameters()):,}\")\n",
    "print(f\"Training for {EPOCHS_BERT} epochs (this will take 1-2 hours!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import time\n",
    "\n",
    "print(\"Training BERT4Rec (this will take 1-2 hours)...\")\n",
    "print(\"=\" * 70)\n",
    "print(\"⚠️  WARNING: This model needs 200+ epochs to work properly!\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "best_loss_bert = float('inf')\n",
    "loss_history_bert = []  # Track loss for visualization\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(1, EPOCHS_BERT + 1):\n",
    "    bert4rec.train()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    for batch_seqs, batch_mask_pos, batch_targets in bert_loader:\n",
    "        batch_seqs = batch_seqs.to(DEVICE)\n",
    "        batch_mask_pos = batch_mask_pos.to(DEVICE)\n",
    "        batch_targets = batch_targets.to(DEVICE)\n",
    "\n",
    "        # Forward pass\n",
    "        logits = bert4rec(batch_seqs, batch_mask_pos)  # [B, num_masks, num_items]\n",
    "\n",
    "        # Reshape for loss calculation\n",
    "        # logits: [B, num_masks, num_items] -> [B*num_masks, num_items]\n",
    "        # targets: [B, num_masks] -> [B*num_masks]\n",
    "        logits_flat = logits.view(-1, logits.size(-1))\n",
    "        targets_flat = batch_targets.view(-1)\n",
    "\n",
    "        # Cross-entropy loss\n",
    "        loss = criterion_bert(logits_flat, targets_flat)\n",
    "\n",
    "        # Backward\n",
    "        optimizer_bert.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping (from original paper)\n",
    "        torch.nn.utils.clip_grad_norm_(bert4rec.parameters(), max_norm=5.0)\n",
    "\n",
    "        optimizer_bert.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "    loss_history_bert.append(avg_loss)  # Track loss\n",
    "    scheduler_bert.step()\n",
    "\n",
    "    # Save best model\n",
    "    if avg_loss < best_loss_bert:\n",
    "        best_loss_bert = avg_loss\n",
    "        torch.save({\n",
    "            \"model_state\": bert4rec.state_dict(),\n",
    "            \"epoch\": epoch,\n",
    "            \"loss\": avg_loss,\n",
    "        }, \"pretrained_models/bert4rec_best.pth\")\n",
    "\n",
    "    # Print progress\n",
    "    if epoch % 10 == 0 or epoch == 1:\n",
    "        elapsed = (time.time() - start_time) / 60\n",
    "        print(f\"Epoch {epoch}/{EPOCHS_BERT} - Loss: {avg_loss:.4f} - Best: {best_loss_bert:.4f} - Time: {elapsed:.1f}min\")\n",
    "\n",
    "# Load best model\n",
    "checkpoint = torch.load(\"pretrained_models/bert4rec_best.pth\")\n",
    "bert4rec.load_state_dict(checkpoint[\"model_state\"])\n",
    "print(f\"\\n✅ Training complete! Best model from epoch {checkpoint['epoch']} with loss {checkpoint['loss']:.4f}\")\n",
    "print(f\"Total training time: {(time.time() - start_time)/60:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot BERT4Rec training loss\n",
    "if 'loss_history_bert' in locals() and len(loss_history_bert) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.plot(loss_history_bert, linewidth=2, color='purple')\n",
    "    ax.set_title('BERT4Rec Training Loss', fontsize=16, fontweight='bold')\n",
    "    ax.set_xlabel('Epoch', fontsize=12)\n",
    "    ax.set_ylabel('Cross-Entropy Loss', fontsize=12)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(f\"Final loss: {loss_history_bert[-1]:.4f}\")\n",
    "    print(f\"Best loss: {min(loss_history_bert):.4f}\")\n",
    "    print(f\"Total epochs: {len(loss_history_bert)}\")\n",
    "else:\n",
    "    print(\"Loss history not available. Please run the training cell first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_bert4rec(model, users, histories, top_k=10, batch_size=1024):\n",
    "    \"\"\"\n",
    "    Generate recommendations using BERT4Rec.\n",
    "\n",
    "    At inference: mask the LAST item and predict it (simulates next-item prediction)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    rankings = {}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(users), batch_size):\n",
    "            batch_users = users[i:i + batch_size]\n",
    "\n",
    "            # Prepare sequences\n",
    "            seqs = []\n",
    "            valid_users = []\n",
    "\n",
    "            for user in batch_users:\n",
    "                hist = histories.get(user)\n",
    "                if not hist:\n",
    "                    continue\n",
    "                encoded = [item2idx[item] for item in hist if item in item2idx]\n",
    "                if not encoded:\n",
    "                    continue\n",
    "\n",
    "                # Take last MAX_SEQ_LEN items\n",
    "                hist_slice = encoded[-MAX_SEQ_LEN:]\n",
    "                padded = [0] * (MAX_SEQ_LEN - len(hist_slice)) + hist_slice\n",
    "\n",
    "                seqs.append(padded)\n",
    "                valid_users.append(user)\n",
    "\n",
    "            if not seqs:\n",
    "                continue\n",
    "\n",
    "            # Convert to tensor\n",
    "            seqs_tensor = torch.tensor(seqs, dtype=torch.long, device=DEVICE)\n",
    "\n",
    "            # Encode sequences (no masking at inference)\n",
    "            encoded = model.transformer(\n",
    "                model.dropout(model.layer_norm(\n",
    "                    model.item_emb(seqs_tensor) +\n",
    "                    model.pos_emb(torch.arange(MAX_SEQ_LEN, device=DEVICE).unsqueeze(0).expand(len(seqs), -1))\n",
    "                )),\n",
    "                src_key_padding_mask=seqs_tensor.eq(0)\n",
    "            )\n",
    "\n",
    "            # Use last position to predict next item\n",
    "            last_hidden = encoded[:, -1, :]  # [B, hidden_dim]\n",
    "            logits = model.out(last_hidden)  # [B, num_items]\n",
    "\n",
    "            # Exclude seen items\n",
    "            for row_idx, user in enumerate(valid_users):\n",
    "                seen = {item2idx[item] for item in histories[user] if item in item2idx}\n",
    "                seen_indices = list(seen)\n",
    "                if seen_indices:\n",
    "                    logits[row_idx, seen_indices] = -1e9\n",
    "\n",
    "            # Get top-k\n",
    "            topk_indices = torch.topk(logits, top_k, dim=1).indices.cpu().numpy()\n",
    "\n",
    "            for user, rec_indices in zip(valid_users, topk_indices):\n",
    "                rec_items = [idx2item[idx] for idx in rec_indices if idx in idx2item]\n",
    "                rankings[user] = rec_items\n",
    "\n",
    "    return rankings\n",
    "\n",
    "print(\"Evaluation function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"Evaluating BERT4Rec model...\")\n",
    "bert4rec_rankings = recommend_bert4rec(\n",
    "    bert4rec,\n",
    "    eval_users,\n",
    "    train_histories_eval,\n",
    "    top_k=EVAL_K\n",
    ")\n",
    "\n",
    "bert_hit = hit_rate_at_k(bert4rec_rankings, test_targets_eval, k=EVAL_K)\n",
    "bert_ndcg = ndcg_at_k(bert4rec_rankings, test_targets_eval, k=EVAL_K)\n",
    "\n",
    "print(f\"\\nBERT4Rec ({len(eval_users)} users) -> Hit@{EVAL_K}: {bert_hit:.4f}, NDCG@{EVAL_K}: {bert_ndcg:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model Comparison\n",
    "\n",
    "Comprehensive comparison of all models evaluated on the same test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive final comparison table\n",
    "all_results = pd.DataFrame([\n",
    "    {\"Model\": \"MostPopular\", f\"Hit@{EVAL_K}\": pop_hit, f\"NDCG@{EVAL_K}\": pop_ndcg},\n",
    "    {\"Model\": \"MarkovChain\", f\"Hit@{EVAL_K}\": mc_hit, f\"NDCG@{EVAL_K}\": mc_ndcg},\n",
    "    {\"Model\": \"ItemKNN\", f\"Hit@{EVAL_K}\": knn_hit, f\"NDCG@{EVAL_K}\": knn_ndcg},\n",
    "    {\"Model\": \"SASRec\", f\"Hit@{EVAL_K}\": sas_hit, f\"NDCG@{EVAL_K}\": sas_ndcg},\n",
    "    {\"Model\": \"Transformer BPR\", f\"Hit@{EVAL_K}\": bpr_hit, f\"NDCG@{EVAL_K}\": bpr_ndcg},\n",
    "    {\"Model\": \"BERT4Rec\", f\"Hit@{EVAL_K}\": bert_hit, f\"NDCG@{EVAL_K}\": bert_ndcg},\n",
    "])\n",
    "\n",
    "# Sort by Hit@10 descending\n",
    "all_results = all_results.sort_values(f'Hit@{EVAL_K}', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"FINAL MODEL COMPARISON - All Models\")\n",
    "print(\"=\" * 80)\n",
    "print(all_results.to_string(index=False))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Visualize final comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Hit@10 comparison\n",
    "ax1 = axes[0]\n",
    "colors = ['steelblue', 'forestgreen', 'coral', 'purple', 'orange', 'red']\n",
    "bars1 = ax1.bar(range(len(all_results)), all_results[f'Hit@{EVAL_K}'], color=colors[:len(all_results)])\n",
    "ax1.set_title(f'All Models - Hit@{EVAL_K} Comparison', fontsize=16, fontweight='bold')\n",
    "ax1.set_ylabel(f'Hit@{EVAL_K}', fontsize=12)\n",
    "ax1.set_xlabel('Model', fontsize=12)\n",
    "ax1.set_xticks(range(len(all_results)))\n",
    "ax1.set_xticklabels(all_results['Model'], rotation=45, ha='right')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "for i, (bar, v) in enumerate(zip(bars1, all_results[f'Hit@{EVAL_K}'])):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, v + 0.001, f'{v:.4f}', \n",
    "             ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "# NDCG@10 comparison\n",
    "ax2 = axes[1]\n",
    "bars2 = ax2.bar(range(len(all_results)), all_results[f'NDCG@{EVAL_K}'], color=colors[:len(all_results)])\n",
    "ax2.set_title(f'All Models - NDCG@{EVAL_K} Comparison', fontsize=16, fontweight='bold')\n",
    "ax2.set_ylabel(f'NDCG@{EVAL_K}', fontsize=12)\n",
    "ax2.set_xlabel('Model', fontsize=12)\n",
    "ax2.set_xticks(range(len(all_results)))\n",
    "ax2.set_xticklabels(all_results['Model'], rotation=45, ha='right')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "for i, (bar, v) in enumerate(zip(bars2, all_results[f'NDCG@{EVAL_K}'])):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, v + 0.0005, f'{v:.4f}', \n",
    "             ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display the table\n",
    "all_results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taichi3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
